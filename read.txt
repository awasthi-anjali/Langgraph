LangGraph Conversational Agent â€” Beginner Notes
ğŸ§  Goal of this program

You want to:

âœ… Keep chat history
âœ… Send user message to Groq LLM
âœ… Store assistant reply back into State
âœ… Run through LangGraph
âœ… Visualize graph
âœ… Stream results

Flow:

5
START
  â†“
SuperBot (LLM)
  â†“
END

ğŸ”¹ 1. State + Reducer (Conversation Memory)
from typing_extensions import TypedDict
from typing import Annotated
from langgraph.graph.message import add_messages

State
class State(TypedDict):
    messages: Annotated[list, add_messages]

âœï¸ Notes (IMPORTANT)
messages

Stores chat history.

Annotated[list, add_messages]

This is called a reducer.

Reducer meaning:

ğŸ‘‰ When nodes return messages, LangGraph MERGES them
ğŸ‘‰ Instead of overwriting, it APPENDS

So conversation grows:

[user â†’ bot â†’ user â†’ bot]


Without reducer, history would be lost.

ğŸ”¹ 2. Environment + API Keys
from dotenv import load_dotenv
load_dotenv()


Loads .env.

os.environ["OPENAI_API_KEY"]
os.environ["GROQ_API_KEY"]


Used by LLM libraries.

ğŸ”¹ 3. LLM Setup
OpenAI (paid)
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(model="gpt-4o")

Groq (free + fast)
from langchain_groq import ChatGroq
llm_groq = ChatGroq(model="llama-3.1-8b-instant")


You use Groq inside SuperBot.

ğŸ”¹ 4. SuperBot Node
Your code:
def superbot(state: State):
    return {"messages":[llm_groq.invoke(state['message'])]} 

âŒ Bug

State contains:

messages


But you used:

state['message']


Singular vs plural.

âœ… Correct SuperBot
def superbot(state: State):
    return {"messages": [llm_groq.invoke(state["messages"])]}

What SuperBot does:

Reads conversation from state["messages"]

Sends to LLM

Gets AI response

Returns as new message

Reducer appends it

ğŸ”¹ 5. Graph Construction
Create graph
graph = StateGraph(State)

Add node
graph.add_node("SuperBot", superbot)


Registers SuperBot.

Add edges
graph.add_edge(START, "SuperBot")
graph.add_edge("SuperBot", END)


Meaning:

START â†’ SuperBot â†’ END

Compile
graph_builder = graph.compile()


Blueprint â†’ runnable graph.

ğŸ”¹ 6. Visualization
display(Image(graph_builder.get_graph().draw_mermaid_png()))


Shows Mermaid diagram.

ğŸ”¹ 7. Invocation (YOU HAD ERRORS HERE)
âŒ Wrong
graph_builder.invoke({'messages':"Hi..."})


Because:

messages MUST be list of chat objects.

âœ… Correct
graph_builder.invoke({
  "messages": [
     {"role":"user","content":"Hi, my name is Anjali and I like badminton"}
  ]
})

ğŸ”¹ 8. Streaming
âŒ Wrong
graph_builder.stream({"messages":"Hello"})

âœ… Correct Streaming
for event in graph_builder.stream(
    {"messages":[{"role":"user","content":"Hello my name is Anjali"}]},
    stream_mode="values"
):
    print(event)

â­ Beginner Rules (WRITE THESE)
âœ… messages must be LIST
âœ… each message = {role, content}
âœ… reducer add_messages keeps history
âœ… keys must match State
âœ… node names are case-sensitive
âœ… invoke input must follow State schema
ğŸ“Œ Clean Final Working Example
class State(TypedDict):
    messages: Annotated[list, add_messages]

def superbot(state: State):
    return {"messages":[llm_groq.invoke(state["messages"])]}

graph = StateGraph(State)

graph.add_node("SuperBot", superbot)
graph.add_edge(START,"SuperBot")
graph.add_edge("SuperBot",END)

graph_builder = graph.compile()

graph_builder.invoke({
 "messages":[{"role":"user","content":"Hi Anjali"}]
})

ğŸ§‘â€ğŸ’¼ Interview Explanation (Short)

Say:

I use TypedDict State with add_messages reducer to maintain conversation. SuperBot node calls LLM and returns messages. StateGraph connects START to SuperBot and END. compile creates executable graph and stream allows real-time output.

ğŸ¯ One-Line Summary

ğŸ‘‰ This builds a LangGraph chat agent with memory using message reducers.